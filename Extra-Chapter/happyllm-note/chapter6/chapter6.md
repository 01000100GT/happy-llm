# Chapter6

## 选择题

### 1. 在LoRA微调中，对于预训练的权重参数矩阵 $ W_0 \in R^{d \times k} $ ，其更新表示为 $ W_0 + {\Delta}W = W_0 + BA $ ，其中 $ B \in R^{d \times r} $ ，$ A \in R^{r \times k} $，在训练过程中（  ）。

A. $W_0$、$A$、$B$都更新

B. $W_0$更新，$A$、$B$冻结

C. $W_0$冻结，$A$、$B$更新

D. $W_0$、$A$、$B$都冻结

答案：C

### 2. 在Transformer结构中，LoRA技术主要应用在注意力模块的四个权重矩阵，消融实验发现同时调整哪两个矩阵会产生最佳结果（  ）。

A. $W_q$和$W_k$

B. $W_q$和$W_v$

C. $W_k$和$W_v$

D. $W_v$和$W_0$

答案：B

### 3. 以下哪种高效微调方法不存在推理延迟问题（  ）。

A. Adapt Tuning

B. Prefix Tuning

C. LoRA

D. Ptuning

答案：C

### 4. 在SFT阶段，模型使用（  ）进行训练。

A. 海量无监督文本

B. 构建成对的指令对数据

C. 单一的文本数据

D. 随机生成的数据

答案：B

### 5. 强化学习中，智能体通过不断与环境交互来优化策略，其交互步骤顺序正确的是（  ）。
### ①观察状态 ②选择动作 ③执行动作 ④接收奖励和新状态 ⑤更新策略

A. ①②③④⑤

B. ②①③④⑤

C. ①③②④⑤

D. ②③①④⑤

答案：A

### 6. 在构建奖励模型的数据集时，不需要做的步骤是（  ）。

A. 收集初始回答

B. 自动生成标注

C. 人工标注与评估

D. 数据格式化与整理

答案：B

### 7. 在LoRA微调中，可训练参数个数 $ \Theta = 2 \times L_{LoRA} \times d_{model} \times r $ ，其中 $r$ 一般取（  ）。

A. 1、2、3

B. 4、8、16

C. 20、30、40

D. 50、100、200

答案：B

### 8. Adapt Tuning方法在微调时（  ）。

A. 冻结原参数，仅更新Adapter层

B. 更新全部参数

C. 冻结Adapter层，更新原参数

D. 部分更新原参数和Adapter层

答案：A

### 9. 在SFT过程中，对于User的文本，在targets中对应的文本内容使用（  ）进行遮蔽。

A. 0

B. 1

C. -100

D. 任意值

答案：C

### 10. 以下关于强化学习目标的说法，正确的是（  ）。

A. 最小化总累计奖励

B. 使智能体随机选择动作

C. 通过在给定环境中反复试探和学习，使得智能体能够选择一系列动作从而最大化其总累计奖励

D. 只考虑短期奖励，不考虑长期奖励

答案：C

## 简答题

1. **请简述LoRA微调的原理。**
答：LoRA假设权重更新的过程中有较低的本征秩，对于预训练的权重参数矩阵 $ W_0 \in R^{d \times k} $ ，使用低秩分解来表示其更新，即 $ W_0 + {\Delta}W = W_0 + BA $ ，其中 $ B \in R^{d \times r} $ ， $ A \in R^{r \times k} $。在训练过程中，$W_0$ 冻结不更新，$A$、$B$ 包含可训练参数。其前向传递函数为 $ h = W_0 x + \Delta W x = W_0 x + B A x $ ，开始训练时，对 $A$ 使用随机高斯初始化，对 $B$ 使用零初始化，然后使用Adam进行优化。在Transformer结构中，LoRA技术主要应用在注意力模块的四个权重矩阵 $W_q$ 、 $W_k$ 、 $W_v$ 、 $W_0$ ，而冻结MLP的权重矩阵，消融实验发现同时调整 $W_q$ 和 $W_v$ 会产生最佳结果。

2. **对比模型预训练（Pretrain）和有监督微调（SFT）的核心差异。**

答：Pretrain和SFT均使用CLM建模，其核心差异在于训练数据和loss计算方式。在Pretrain阶段，会对海量无监督文本进行自监督建模，来学习文本语义规则和文本中的世界知识，对全部text进行loss计算，要求模型对整个文本实现建模预测；而在SFT阶段，一般通过对Pretrain好的模型进行指令微调，即训练模型根据用户指令完成对应任务，从而使模型能够遵循用户指令，根据用户指令进行规划、行动和输出，使用构建成对的指令对数据，仅对输出进行loss计算，不计算指令部分的loss。

3. **简述强化学习的基本原理和目标。**

答：强化学习的基本原理涉及状态、动作、奖励、策略、价值函数和模型等元素。状态是系统在某一时刻的具体状况；动作是智能体在给定状态下可执行的操作；奖励是智能体执行动作后获得的反馈；策略是指导智能体选择动作的规则；价值函数是评估策略的工具，预测从当前状态出发长期能获得的总奖励；模型可帮助智能体预见动作结果。智能体通过观察状态、选择动作、执行动作、接收奖励和新状态、更新策略这几个步骤与环境进行交互，不断优化策略。其目标是通过在给定环境中反复试探和学习，使得智能体能够选择一系列动作从而最大化其总累计奖励，在数学上表示为训练一个策略$\pi$，使得在所有状态$s$下，智能体选择的动作能够使得回报$R(\tau)$的期望值最大化，通过梯度上升的方法不断更新策略参数$\theta$。

4. **请说明构建奖励模型数据集的步骤。**

答：构建奖励模型数据集可按以下步骤进行：首先收集初始回答，从一个已经过基本微调的“大模型”中，为一组精心设计的提示生成多条回答；然后进行人工标注与评估，邀请专业标注人员或众包标注者对每条回答的质量进行评价，基于一系列预先设计的评价标准，如回答的准确性、完整性、上下文相关性、语言流畅度以及是否遵循道德与安全准则，对不同回答进行比较与排序；最后进行数据格式化与整理，将标注完成的数据进行整理与格式化，采用JSON、CSV等便于计算机处理的结构化数据格式，明确标识每个问题、其对应的多个回答，以及人类标注者对这些回答的选择，如标记为 "chosen" 的最佳答案与 "rejected" 的较差答案。

5. **简述使用peft实现LoRA微调的步骤及注意事项。**

答：使用peft实现LoRA微调，首先通过`get_peft_model`获取一个LoRA模型，此处的`get_peft_model`底层操作有具体实现。然后使用transformers提供的Trainer进行训练，示例代码如下：

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset= IterableWrapper(train_dataset),
    tokenizer=tokenizer
)
trainer.train()
```

如果应用在DPO、KTO上，相同的加入LoRA参数并通过`get_peft_model`获取LoRA模型，其他不需要修改。注意事项是，LoRA微调能够大幅度降低显卡占用，且在下游任务适配上能够取得较好的效果，但如果是需要学习对应知识的任务，LoRA由于只调整低秩矩阵，难以实现知识的注入，一般效果不佳，因此不推荐使用LoRA进行模型预训练或后训练。 