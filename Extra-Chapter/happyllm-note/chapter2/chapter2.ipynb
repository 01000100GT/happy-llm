{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8f84d6",
   "metadata": {},
   "source": [
    "# 第二章：Transformer架构\n",
    "\n",
    "本章节将会结合正文当中的Transformer架构的知识，为大家实际动手操作一下，如何应用Transformer架构做一个中英翻译器，并且最终保存为权重共大家进行推导集的实验\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedb3ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import jieba  \n",
    "import re  \n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9da891",
   "metadata": {},
   "source": [
    "## 读取本地数据集\n",
    "\n",
    "数据集为english.en（英文句子）和chinese.zh（对应中文翻译），每行一条数据，需保证两条文件的句子一一对应："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcec0d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据描述\n",
      "数据集大小: 10000\n",
      "数据集前5行:\n",
      "                                             chinese  \\\n",
      "0                                      1929年还是1989年?   \n",
      "1  巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...   \n",
      "2  一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...   \n",
      "3  如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...   \n",
      "4                  目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。   \n",
      "\n",
      "                                             english  \n",
      "0                                      1929 or 1989?  \n",
      "1  PARIS – As the economic crisis deepens and wid...  \n",
      "2  At the start of the crisis, many people likene...  \n",
      "3  Today, the mood is much grimmer, with referenc...  \n",
      "4  The tendency is either excessive restraint (Eu...  \n"
     ]
    }
   ],
   "source": [
    "def load_data(chinese_path, english_path, max_samples=10000):\n",
    "    \"\"\"加载中文和英文数据，最多加载max_samples条\"\"\"\n",
    "    with open(chinese_path, 'r', encoding='utf-8') as f:\n",
    "        chinese_data = f.readlines()  \n",
    "    with open(english_path, 'r', encoding='utf-8') as f:\n",
    "        english_data = f.readlines()\n",
    "    \n",
    "    # 确保两种语言数据量一致，并限制数量\n",
    "    min_len = min(len(chinese_data), len(english_data), max_samples)\n",
    "    chinese_data = chinese_data[:min_len]\n",
    "    english_data = english_data[:min_len]\n",
    "    \n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'chinese': chinese_data,\n",
    "        'english': english_data\n",
    "    })\n",
    "    \n",
    "    # 去除每行末尾的换行符\n",
    "    df['chinese'] = df['chinese'].str.strip()\n",
    "    df['english'] = df['english'].str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "chinese_path = 'chinese.zh'\n",
    "english_path = 'english.en'\n",
    "# 加载数据\n",
    "print(\"数据描述\")\n",
    "data = load_data(chinese_path, english_path)\n",
    "print(f\"数据集大小: {len(data)}\")\n",
    "print(f\"数据集前5行:\\n{data.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa5fa7",
   "metadata": {},
   "source": [
    "## 分词处理\n",
    "\n",
    "**1.** 中文分词：用jieba实现，对应文档 1.3.1 节 “中文分词”（解决中文无空格分隔问题）。\n",
    "\n",
    "**2.** 英文分词：用正则表达式分割单词和标点，对应文档 1.3.2 节 “子词切分”（将文本拆分为最小语义单位）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb4b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\xnlll\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文原句：1929年还是1989年?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.474 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文分词：['1929', '年', '还是', '1989', '年', '?']\n",
      "英文原句：1929 or 1989?\n",
      "英文分词：['1929', 'or', '1989', '?']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_zh(text):\n",
    "    \"\"\"中文分词：保留标点，返回token列表\"\"\"\n",
    "    return list(jieba.cut(text))  # jieba.cut返回生成器，转为列表\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"英文分词：用正则分割单词和标点，转为小写\"\"\"\n",
    "    # 匹配字母、数字、 apostrophe（如don't）和标点\n",
    "    pattern = re.compile(r\"[a-zA-Z0-9']+|[^\\w\\s]\")\n",
    "    tokens = pattern.findall(text.lower())  # 小写统一格式\n",
    "    return tokens\n",
    "\n",
    "# 示例：测试分词效果\n",
    "sample = data.iloc[0]\n",
    "print(f\"中文原句：{sample['chinese']}\")\n",
    "print(f\"中文分词：{tokenize_zh(sample['chinese'])}\")\n",
    "print(f\"英文原句：{sample['english']}\")\n",
    "print(f\"英文分词：{tokenize_en(sample['english'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed65195",
   "metadata": {},
   "source": [
    "## 构建词汇表\n",
    "\n",
    "对应文档中的“Embedding 层”内容\n",
    "\n",
    "词汇表是将 token 映射为索引的关键，用于 Embedding 层查找词向量。手动实现词汇表类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89eae687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文词汇表大小：9821，英文词汇表大小：9082\n",
      "中文特殊符号索引：<bos>2，<eos>3，<pad>1\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens_list, min_freq=2, specials=['<unk>', '<pad>', '<bos>', '<eos>']):\n",
    "        \"\"\"\n",
    "        tokens_list: 所有句子的分词结果列表（如[[token1, token2], ...]）\n",
    "        min_freq: 过滤低频词（出现次数<min_freq的token视为<unk>）\n",
    "        specials: 特殊符号（未知词、填充、句首、句尾）\n",
    "        \"\"\"\n",
    "        # 统计token频率\n",
    "        token_counts = Counter([token for seq in tokens_list for token in seq])\n",
    "        # 筛选高频词（保留出现次数≥min_freq的token）\n",
    "        self.token_list = [token for token, cnt in token_counts.items() if cnt >= min_freq]\n",
    "        # 加入特殊符号（放在最前面，保证索引固定）\n",
    "        self.token_list = specials + self.token_list\n",
    "        # 构建token→索引映射\n",
    "        self.token2idx = {token: idx for idx, token in enumerate(self.token_list)}\n",
    "        # 构建索引→token映射\n",
    "        self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_list)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"将token列表转为索引列表（未知token用<unk>的索引）\"\"\"\n",
    "        return [self.token2idx.get(token, self.token2idx['<unk>']) for token in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"将索引列表转为token列表\"\"\"\n",
    "        return [self.idx2token[idx] for idx in ids]\n",
    "\n",
    "# 生成所有句子的分词结果，用于构建词汇表\n",
    "zh_tokens_all = [tokenize_zh(text) for text in data['chinese']]  # 所有中文句子的分词结果\n",
    "en_tokens_all = [tokenize_en(text) for text in data['english']]  # 所有英文句子的分词结果\n",
    "\n",
    "# 构建中、英文词汇表\n",
    "zh_vocab = Vocab(zh_tokens_all, min_freq=2)  # 中文词汇表\n",
    "en_vocab = Vocab(en_tokens_all, min_freq=2)  # 英文词汇表\n",
    "\n",
    "print(f\"中文词汇表大小：{len(zh_vocab)}，英文词汇表大小：{len(en_vocab)}\")\n",
    "print(f\"中文特殊符号索引：<bos>{zh_vocab.token2idx['<bos>']}，<eos>{zh_vocab.token2idx['<eos>']}，<pad>{zh_vocab.token2idx['<pad>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c397c",
   "metadata": {},
   "source": [
    "## 构建数据集与数据加载器\n",
    "\n",
    "对应文档 “序列数据处理”\n",
    "\n",
    "将文本转为模型可输入的张量，添加<bos>（句首）和<eos>（句尾）符号，并对齐序列长度（用<pad>填充）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ae7ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源语言批次形状：torch.Size([32, 57])，目标语言批次形状：torch.Size([32, 53])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, df, src_vocab, tgt_vocab, src_col='english', tgt_col='chinese'):\n",
    "        \"\"\"\n",
    "        df: 包含源语言和目标语言的DataFrame\n",
    "        src_vocab: 源语言（英文）词汇表\n",
    "        tgt_vocab: 目标语言（中文）词汇表\n",
    "        src_col/tgt_col: DataFrame中源/目标语言列名\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.src_col = src_col\n",
    "        self.tgt_col = tgt_col\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取句子\n",
    "        src_text = self.df.iloc[idx][self.src_col]\n",
    "        tgt_text = self.df.iloc[idx][self.tgt_col]\n",
    "        \n",
    "        # 分词\n",
    "        src_tokens = tokenize_en(src_text)  # 英文分词\n",
    "        tgt_tokens = tokenize_zh(tgt_text)  # 中文分词\n",
    "        \n",
    "        # 添加句首（<bos>）和句尾（<eos>）符号\n",
    "        src_tokens = ['<bos>'] + src_tokens + ['<eos>']\n",
    "        tgt_tokens = ['<bos>'] + tgt_tokens + ['<eos>']\n",
    "        \n",
    "        # 转为索引\n",
    "        src_ids = self.src_vocab.convert_tokens_to_ids(src_tokens)\n",
    "        tgt_ids = self.tgt_vocab.convert_tokens_to_ids(tgt_tokens)\n",
    "        \n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "# 定义collate_fn：将批次内序列填充至同一长度\n",
    "def collate_fn(batch, pad_idx_src, pad_idx_tgt):\n",
    "    \"\"\"\n",
    "    batch: 数据集返回的(src_ids, tgt_ids)列表\n",
    "    pad_idx_src: 源语言<pad>的索引\n",
    "    pad_idx_tgt: 目标语言<pad>的索引\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    # 填充源语言序列（batch_first=True表示输出形状为[batch_size, seq_len]）\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_idx_src)\n",
    "    # 填充目标语言序列\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=pad_idx_tgt)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "# 初始化数据集\n",
    "dataset = TranslationDataset(\n",
    "    df=data,\n",
    "    src_vocab=en_vocab,\n",
    "    tgt_vocab=zh_vocab\n",
    ")\n",
    "\n",
    "# 初始化数据加载器\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collate_fn(\n",
    "        x,\n",
    "        pad_idx_src=en_vocab.token2idx['<pad>'],\n",
    "        pad_idx_tgt=zh_vocab.token2idx['<pad>']\n",
    "    )\n",
    ")\n",
    "\n",
    "# 测试数据加载器\n",
    "src_sample, tgt_sample = next(iter(dataloader))\n",
    "print(f\"源语言批次形状：{src_sample.shape}，目标语言批次形状：{tgt_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbc8da",
   "metadata": {},
   "source": [
    "## 构建 Transformer 模型（核心）\n",
    "\n",
    "### 1.位置编码\n",
    "\n",
    "注意力机制本身不包含位置信息，需通过正余弦函数注入序列顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcb59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 初始化位置编码矩阵（max_seq_len, d_model）\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        # 位置索引（0到max_seq_len-1）\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算频率因子（避免数值过大）\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 偶数维度用正弦，奇数维度用余弦\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 增加batch维度（1, max_seq_len, d_model）\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)  # 不参与训练的缓冲区\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x: 输入序列嵌入，形状为[batch_size, seq_len, d_model]\"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]  # 加位置编码（自动广播batch维度）\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04c1f3",
   "metadata": {},
   "source": [
    "### 注意力机制\n",
    "\n",
    "实现核心公式 \n",
    "\n",
    "$$\n",
    "attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2790ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    query: [batch_size, n_heads, seq_len_q, d_k]\n",
    "    key: [batch_size, n_heads, seq_len_k, d_k]\n",
    "    value: [batch_size, n_heads, seq_len_v, d_v]\n",
    "    mask: [batch_size, 1, seq_len_q, seq_len_k]（1表示可见，0表示遮蔽）\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    # 计算相似度分数：Q*K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # 应用掩码（遮蔽<pad>或未来信息）\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)  # 掩码位置设为负无穷\n",
    "    # softmax归一化得到注意力权重\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    # 应用dropout\n",
    "    if dropout is not None:\n",
    "        attn_weights = dropout(attn_weights)\n",
    "    # 加权求和得到输出\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737776ee",
   "metadata": {},
   "source": [
    "### 多头注意力\n",
    "\n",
    "将注意力拆分为多个头并行计算，捕捉不同语义关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25717cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model必须是n_heads的整数倍\"\n",
    "        self.d_k = d_model // n_heads  # 每个头的维度\n",
    "        self.n_heads = n_heads\n",
    "        # Q/K/V的线性变换矩阵（将d_model映射到d_model）\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        # 输出线性变换矩阵（将多头结果拼接后映射回d_model）\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query/key/value: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        # 线性变换并拆分多头：[batch_size, seq_len, d_model] → [batch_size, n_heads, seq_len, d_k]\n",
    "        q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        # 计算注意力\n",
    "        output, attn_weights = attention(q, k, v, mask, self.dropout)\n",
    "        # 拼接多头结果：[batch_size, n_heads, seq_len, d_k] → [batch_size, seq_len, d_model]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # 输出线性变换\n",
    "        return self.w_o(output), attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c8f24b",
   "metadata": {},
   "source": [
    "### 前馈网络与层归一化\n",
    "\n",
    "- 前馈网络（FFN）：增强模型非线性能力，对每个位置独立处理。\n",
    "\n",
    "- 层归一化 + 残差连接：稳定训练，缓解深层模型梯度问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61b8621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)  # 升维\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)  # 降维\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu  # 非线性激活\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.activation(self.fc1(x))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))  # 缩放参数\n",
    "        self.beta = nn.Parameter(torch.zeros(features))   # 偏移参数\n",
    "        self.eps = eps  # 防止除零\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)  # 最后一维求均值\n",
    "        std = x.std(-1, keepdim=True)    # 最后一维求标准差\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f4044",
   "metadata": {},
   "source": [
    "### Encoder 与 Decoder 层\n",
    "\n",
    "- Encoder 层：由 “多头自注意力 + 前馈网络” 组成，输出源语言编码。\n",
    "\n",
    "- Decoder 层：由 “掩码多头自注意力（遮蔽未来信息）+ 编码器 - 解码器注意力（关联源 - 目标语言）+ 前馈网络” 组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddaddaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)  # 自注意力\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)  # 前馈网络\n",
    "        self.norm1 = LayerNorm(d_model)  # 层归一化1\n",
    "        self.norm2 = LayerNorm(d_model)  # 层归一化2\n",
    "        self.dropout1 = nn.Dropout(dropout)  #  dropout1\n",
    "        self.dropout2 = nn.Dropout(dropout)  #  dropout2\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # 自注意力 + 残差连接 + 层归一化\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        # 前馈网络 + 残差连接 + 层归一化\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)  # 掩码自注意力\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)  # 编码器-解码器注意力\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)  # 前馈网络\n",
    "        self.norm1 = LayerNorm(d_model)  # 层归一化1\n",
    "        self.norm2 = LayerNorm(d_model)  # 层归一化2\n",
    "        self.norm3 = LayerNorm(d_model)  # 层归一化3\n",
    "        self.dropout1 = nn.Dropout(dropout)  #  dropout1\n",
    "        self.dropout2 = nn.Dropout(dropout)  #  dropout2\n",
    "        self.dropout3 = nn.Dropout(dropout)  #  dropout3\n",
    "    \n",
    "    def forward(self, x, enc_output, self_mask, cross_mask):\n",
    "        # 掩码自注意力（遮蔽未来信息）\n",
    "        attn_output, _ = self.self_attn(x, x, x, self_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        # 编码器-解码器注意力（关联源语言和目标语言）\n",
    "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        # 前馈网络\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd48e77",
   "metadata": {},
   "source": [
    "### 组装成完整 Transformer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_layers=6, \n",
    "                 n_heads=8, d_ff=2048, max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 编码器\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # 源语言Embedding\n",
    "        self.encoder_pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        # 解码器\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  # 目标语言Embedding\n",
    "        self.decoder_pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        # 输出层（映射到目标语言词汇表）\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.d_model = d_model  # 模型维度\n",
    "    \n",
    "    def generate_masks(self, src, tgt):\n",
    "        \"\"\"生成编码器和解码器掩码\"\"\"\n",
    "        batch_size, src_len = src.shape\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # 编码器掩码：遮蔽<pad>（src中<pad>的位置设为0）\n",
    "        src_mask = (src != en_vocab.token2idx['<pad>']).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, src_len]\n",
    "        \n",
    "        # 解码器自注意力掩码：遮蔽<pad>和未来信息（下三角矩阵）\n",
    "        tgt_self_mask = (tgt != zh_vocab.token2idx['<pad>']).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len]\n",
    "        \n",
    "        # 确保下三角矩阵为布尔类型\n",
    "        tgt_tri_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device, dtype=torch.bool))\n",
    "        \n",
    "        # 确保两个掩码都是布尔类型再进行位运算\n",
    "        tgt_self_mask = tgt_self_mask & tgt_tri_mask  # 结合两种掩码\n",
    "        \n",
    "        # 解码器交叉注意力掩码：与编码器掩码一致（确保对齐源语言）\n",
    "        tgt_cross_mask = src_mask\n",
    "        \n",
    "        return src_mask, tgt_self_mask, tgt_cross_mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: 源语言序列（英文），形状[batch_size, src_len]\n",
    "        tgt: 目标语言序列（中文），形状[batch_size, tgt_len]（训练时不含最后一个<eos>）\n",
    "        \"\"\"\n",
    "        # 生成掩码\n",
    "        src_mask, tgt_self_mask, tgt_cross_mask = self.generate_masks(src, tgt)\n",
    "        \n",
    "        # 编码器前向\n",
    "        enc_emb = self.encoder_embedding(src) * math.sqrt(self.d_model)  # Embedding缩放\n",
    "        enc_emb = self.encoder_pos_encoding(enc_emb)  # 加位置编码\n",
    "        enc_output = enc_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)  \n",
    "        \n",
    "        # 解码器前向\n",
    "        dec_emb = self.decoder_embedding(tgt) * math.sqrt(self.d_model)  # Embedding缩放\n",
    "        dec_emb = self.decoder_pos_encoding(dec_emb)  # 加位置编码\n",
    "        dec_output = dec_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, tgt_self_mask, tgt_cross_mask)  # 经过所有解码器层\n",
    "        \n",
    "        # 输出层（映射到目标词汇表）\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5062137",
   "metadata": {},
   "source": [
    "## 模型训练与训练集保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50d8c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "d_model = 512  # 模型维度（原论文512）\n",
    "n_layers = 6  # 编码器/解码器层数（原论文6）\n",
    "n_heads = 8   # 注意力头数（原论文8）\n",
    "d_ff = 2048   # 前馈网络隐藏层维度（原论文2048）\n",
    "dropout = 0.1\n",
    "epochs = 30   # 训练轮数\n",
    "lr = 0.0001   # 学习率\n",
    "\n",
    "# 初始化模型\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(en_vocab),\n",
    "    tgt_vocab_size=len(zh_vocab),\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# 损失函数（忽略<pad>的损失）\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=zh_vocab.token2idx['<pad>'])\n",
    "\n",
    "# 优化器（Adam）\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207b041",
   "metadata": {},
   "source": [
    "准备完成之后开始训练我们自己的模型啦！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2626f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 平均损失: 6.5164\n",
      "\n",
      "Epoch 2 平均损失: 5.9108\n",
      "\n",
      "Epoch 3 平均损失: 5.5808\n",
      "\n",
      "Epoch 4 平均损失: 5.2947\n",
      "\n",
      "Epoch 5 平均损失: 5.0408\n",
      "\n",
      "Epoch 6 平均损失: 4.8072\n",
      "\n",
      "Epoch 7 平均损失: 4.5925\n",
      "\n",
      "Epoch 8 平均损失: 4.3832\n",
      "\n",
      "Epoch 9 平均损失: 4.1815\n",
      "\n",
      "Epoch 10 平均损失: 3.9823\n",
      "\n",
      "Epoch 11 平均损失: 3.7903\n",
      "\n",
      "Epoch 12 平均损失: 3.5957\n",
      "\n",
      "Epoch 13 平均损失: 3.4023\n",
      "\n",
      "Epoch 14 平均损失: 3.2126\n",
      "\n",
      "Epoch 15 平均损失: 3.0264\n",
      "\n",
      "Epoch 16 平均损失: 2.8420\n",
      "\n",
      "Epoch 17 平均损失: 2.6566\n",
      "\n",
      "Epoch 18 平均损失: 2.4752\n",
      "\n",
      "Epoch 19 平均损失: 2.2976\n",
      "\n",
      "Epoch 20 平均损失: 2.1292\n",
      "\n",
      "Epoch 21 平均损失: 1.9595\n",
      "\n",
      "Epoch 22 平均损失: 1.8008\n",
      "\n",
      "Epoch 23 平均损失: 1.6442\n",
      "\n",
      "Epoch 24 平均损失: 1.4992\n",
      "\n",
      "Epoch 25 平均损失: 1.3558\n",
      "\n",
      "Epoch 26 平均损失: 1.2265\n",
      "\n",
      "Epoch 27 平均损失: 1.1065\n",
      "\n",
      "Epoch 28 平均损失: 0.9931\n",
      "\n",
      "Epoch 29 平均损失: 0.8851\n",
      "\n",
      "Epoch 30 平均损失: 0.7962\n",
      "\n",
      "模型权重已保存至 'transformer_zh_en.pth'\n"
     ]
    }
   ],
   "source": [
    "# 设备选择（GPU优先）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 训练模式\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "        # 移动数据到设备\n",
    "        src = src.to(device)  # [batch_size, src_len]\n",
    "        tgt = tgt.to(device)  # [batch_size, tgt_len]\n",
    "        \n",
    "        # 解码器输入：tgt[:, :-1]（去除最后一个<eos>）\n",
    "        # 解码器目标：tgt[:, 1:]（去除第一个<bos>）\n",
    "        output = model(src, tgt[:, :-1])  # [batch_size, tgt_len-1, tgt_vocab_size]\n",
    "        \n",
    "        # 计算损失（CrossEntropyLoss要求输入形状为[batch, vocab_size, seq_len]）\n",
    "        loss = criterion(output.transpose(1, 2), tgt[:, 1:])\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # 清零梯度\n",
    "        loss.backward()        # 计算梯度\n",
    "        optimizer.step()       # 更新参数\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 打印轮次平均损失\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} 平均损失: {avg_loss:.4f}\\n\")\n",
    "\n",
    "# 保存模型权重\n",
    "torch.save(model.state_dict(), 'transformer_zh_en.pth')\n",
    "print(\"模型权重已保存至 'transformer_zh_en.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d36bcc",
   "metadata": {},
   "source": [
    "## 使用预训练集进行推导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be2f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 中英文翻译器 =====\n",
      "提示：输入英文句子进行翻译，输入'q'退出\n",
      "翻译结果: 巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望我们了解目前正在发生的情况。\n",
      "翻译结果: 但如果这样的情况发生在危机前<unk>，那么有许多国家的人真正稳定自己的那么重要，那就是自己的自己的自己国家，就会有自己的自己的自己的吗？\n",
      "程序已退出\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence, vocab, tokenize_fn):\n",
    "    \"\"\"预处理输入句子，转为模型可接受的格式\"\"\"\n",
    "    tokens = tokenize_fn(sentence)  # 分词\n",
    "    tokens = ['<bos>'] + tokens + ['<eos>']  # 添加句首/句尾符号\n",
    "    indices = vocab.convert_tokens_to_ids(tokens)  # 转为索引\n",
    "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # 添加batch维度\n",
    "    return tensor\n",
    "\n",
    "def translate_sentence(model, src_sentence, src_vocab, tgt_vocab, \n",
    "                       src_tokenize_fn, tgt_tokenize_fn, device, max_len=50):\n",
    "    \"\"\"将英文句子翻译成中文\"\"\"\n",
    "    model.eval()  # 评估模式，关闭dropout等训练特有的层\n",
    "    with torch.no_grad():  # 关闭梯度计算，节省内存\n",
    "        # 预处理源语言句子\n",
    "        src_tensor = preprocess_sentence(src_sentence, src_vocab, src_tokenize_fn).to(device)\n",
    "        \n",
    "        # 编码器处理源语言\n",
    "        src_mask = (src_tensor != src_vocab.token2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "        enc_emb = model.encoder_embedding(src_tensor) * math.sqrt(model.d_model)\n",
    "        enc_emb = model.encoder_pos_encoding(enc_emb)\n",
    "        enc_output = enc_emb\n",
    "        for layer in model.encoder_layers:\n",
    "            enc_output = layer(enc_output, src_mask)\n",
    "        \n",
    "        # 解码器自回归生成目标句子（从<bos>开始）\n",
    "        tgt_indices = [tgt_vocab.token2idx['<bos>']]\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 生成解码器掩码（遮蔽未来信息和<pad>）\n",
    "            tgt_self_mask = (tgt_tensor != tgt_vocab.token2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "            tgt_tri_mask = torch.tril(\n",
    "                torch.ones(tgt_tensor.size(1), tgt_tensor.size(1), device=device, dtype=torch.bool)\n",
    "            )\n",
    "            tgt_self_mask = tgt_self_mask & tgt_tri_mask\n",
    "            tgt_cross_mask = src_mask  # 与编码器掩码一致\n",
    "            \n",
    "            # 解码器前向计算\n",
    "            dec_emb = model.decoder_embedding(tgt_tensor) * math.sqrt(model.d_model)\n",
    "            dec_emb = model.decoder_pos_encoding(dec_emb)\n",
    "            dec_output = dec_emb\n",
    "            for layer in model.decoder_layers:\n",
    "                dec_output = layer(dec_output, enc_output, tgt_self_mask, tgt_cross_mask)\n",
    "            output = model.fc(dec_output)\n",
    "\n",
    "            # 取最后一个位置的预测结果\n",
    "            next_token_idx = output[:, -1, :].argmax(1).item()\n",
    "            tgt_indices.append(next_token_idx)\n",
    "            \n",
    "            # 遇到<eos>停止生成\n",
    "            if next_token_idx == tgt_vocab.token2idx['<eos>']:\n",
    "                break\n",
    "        \n",
    "        # 转换为中文句子（移除特殊符号）\n",
    "        tgt_tokens = tgt_vocab.convert_ids_to_tokens(tgt_indices)\n",
    "        translated_sentence = ''.join(tgt_tokens[1:-1])  # 跳过<bos>和<eos>\n",
    "        return translated_sentence\n",
    "\n",
    "# 加载模型（需与训练时的超参数一致）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "# 初始化模型并加载权重\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(en_vocab),\n",
    "    tgt_vocab_size=len(zh_vocab),\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load('transformer_zh_en.pth', map_location=device))\n",
    "\n",
    "# 交互翻译\n",
    "print(\"===== 中英文翻译器 =====\")\n",
    "print(\"提示：输入英文句子进行翻译，输入'q'退出\")\n",
    "while True:\n",
    "    user_input = input(\"\\n请输入英文句子: \")\n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"程序已退出\")\n",
    "        break\n",
    "    # 执行翻译\n",
    "    translation = translate_sentence(\n",
    "        model=model,\n",
    "        src_sentence=user_input,\n",
    "        src_vocab=en_vocab,\n",
    "        tgt_vocab=zh_vocab,\n",
    "        src_tokenize_fn=tokenize_en,  # 英文分词函数\n",
    "        tgt_tokenize_fn=tokenize_zh,  # 中文分词函数\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"翻译结果: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
